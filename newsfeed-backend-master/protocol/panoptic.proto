syntax = "proto3";
package protocol;

import "google/protobuf/timestamp.proto";

option go_package = "github.com/rnr-capital/newsfeed-backend/publisher/protocol";

message KeyValuePair {
  string key = 1;
  string value = 2;
}

message PanopticJob {
  // Job id uniquely identifies a data collection job, which contains multiple 
  // heterogeneous tasks collecting data from multiple sources. 
  string job_id = 1;

  // Multiple heterogeneous tasks this job contains.
  repeated PanopticTask tasks = 2;

  // Whether this job is a debug only job. A debug only job will not call Lambda
  // at all.
  bool debug = 3;
}

// Used for testing, where multiple jobs can be parse at once.
message PanopticJobs {
  repeated PanopticJob jobs = 1;
}

// TaskParams defines the shared/domain specific parameters that customize the 
// execution behavior.
message TaskParams {
  // If specified, append these header params to the crawler request, 
  // overwrite on same key.
  repeated KeyValuePair header_params = 1;
  
  // If specified, overwrite Cookie with the provided cookies.
  repeated KeyValuePair cookies = 2;

  // SourceId of this collected CrawlerMessage.
  string source_id = 3;

  repeated PanopticSubSource sub_sources = 4;

  // Used in scheduler, when multiple subsources are presented in a single 
  // PanopticTask, it should split it into multiple scheduler jobs, each 
  // contains at most ${max_subsource_per_task} sub sources.
  int64 max_subsource_per_task = 5;

  // offsite of domain specific param from 20, <20 is for shared fields
  // Domain specific params that will be passed in to customize the task 
  // execution. For example, you'll pass in Weibo/Twitter/ZSXQ user id as part
  // of the task param.
  oneof params {
    JinshiTaskParams jinshi_task_params = 20;
    WeiboTaskParams weibo_task_params = 21;
    ZsxqTaskParams zsxq_task_params = 22;
    WallstreetNewsTaskParams wallstreet_news_task_params = 23; 
    WisburgParams wisburg_task_params = 24;
    CaUsNewsTaskParams caus_news_task_params = 25;
    // This param is defined for customized source(not subsource level), if collector id === COLLECTOR_USER_CUSTOMIZED_SOURCE
    CustomizedCrawlerParams customized_source_crawler_task_params = 26;
    Wublock123TaskParams wublock123_task_params = 27;
  }
}

message TaskMetadata {
  // name of the config that triggers this task.
  string config_name = 1;

  // job_start/end_time describes the execution span of this task.
  google.protobuf.Timestamp task_start_time = 2;
  google.protobuf.Timestamp task_end_time = 3;

  // How many CrawlerMessage this task collected.
  int32 total_message_collected = 4;

  // How many CrawlerMessage this task failed to collect.
  int32 total_message_failed = 5;

  // Which ip address is this task executing at.
  string ip_addr = 6;

  // The result state of the task execution, determined by data collector.
  enum TaskResultState {
    STATE_UNSPECIFIED = 0;
    STATE_SUCCESS = 1;
    STATE_FAILURE = 2;
  }

  TaskResultState result_state = 7;

  // How many CrawlerMessage this task skipeed to collect, eg. ads, request subsource different from crawled
	int32 total_message_skipped = 8;
  
  // ...
}

// PanopticTask defines a single data collection task for a single source. A 
// task is the smallest execution in Lambda.
message PanopticTask {
  // UUID for this task.
  string task_id = 1;

  // DataCollectorId defines the data collection logic to execute.
  enum DataCollectorId {
    COLLECTOR_UNSPECIFIED = 0;
    COLLECTOR_JINSHI = 1;
    COLLECTOR_KUAILANSI = 2;
    COLLECTOR_WEIBO = 3;
    COLLECTOR_ZSXQ = 4;
    COLLECTOR_WALLSTREET_NEWS= 5;
    COLLECTOR_JINSE = 6;
    COLLECTOR_CAUS_ARTICLE= 7;
    COLLECTOR_WISBURG = 8;
    COLLECTOR_KR36 = 9;
    COLLECTOR_WEIXIN_ARTICLE= 10;
    COLLECTOR_CAUS_NEWS= 11;
    COLLECTOR_CAIXIN = 12;
    COLLECTOR_WALLSTREET_ARTICLE = 13;
    COLLECTOR_CLS_NEWS=14;
    COLLECTOR_GELONGHUI_NEWS = 15;
    COLLECTOR_USER_CUSTOMIZED_SOURCE = 16;
    COLLECTOR_USER_CUSTOMIZED_SUBSOURCE = 17;
    COLLECTOR_TWITTER = 18;
    COLLECTOR_XUEQIU = 19;
    COLLECTOR_WUBLOCK123 = 20;
    // ...
  }

  // The mapping from this ID to corresponding collector is hard coded
  DataCollectorId data_collector_id = 2;

  // Params that customizes how to collect data from the web.
  TaskParams task_params = 3;

  // Metadata that's mostly for monitoring.
  TaskMetadata task_metadata = 4;
}

message PanopticSubSource{ 
  // example: 
  // name: 快讯
  // type: FLASHNEWS 
  // external_id: None 

  // example:
  // name: 要闻 
  // type: KEYNEWS 
  // external_id: None 

  // example:
  // name: 加州分析员 
  // type: USERS 
  // external_id:15281511824122 
  enum SubSourceType {
    UNSPECIFIED = 0;
    FLASHNEWS = 1;
    KEYNEWS = 2;
    USERS = 3;
    ARTICLE = 4;
    // Sometimes a website can have multiple channels, each can be parsed with 
    // the same set of selectors. (e.g. Caixin is using CHANNEL for choose
    // subsource)
    CHANNEL = 5;
  }

  // name of the sub source, this will be written to CrawlerMessage
  string name = 1;

  // type of the sub source, this will be used to infer:
  // 1. for SUBSOURCE_USERS: DataCollector will hard code what url to crawl
  // 1. for news : DataCollector will determine keynews or flashnews and populate specified one 
  SubSourceType type = 2;

  // this will be used to construct external request/crawl uri
  string external_id = 3;

  // Optional. Sometimes each subsource can have different url.
  string link = 4;

  // This used to define a sub source (not source level) level customized crawler.
  // This kind of subsource is only for a predefined source - with collector id === COLLECTOR_USER_CUSTOMIZED_SUBSOURCE 
  optional CustomizedCrawlerParams customized_crawler_params_for_sub_source = 5;

  optional string avatar_url = 6;
}

// Created empty param here in case we need to pass in additional parameters to
// customize Jinshi's crawler logic.
// skip_key_words: if content contains the following keywords, skip the message
//                 used for ads
message JinshiTaskParams {
  repeated string skip_key_words = 1;
}

// 1. max_pages: As weibo API is paginated, this specifies how many pages does collector look back
//    in each task.
message WeiboTaskParams {
  int32 max_pages = 1;
}

// 1. list channels to crawl
// 2. number of news items in each crawled API
// Example : WallstreetNewsTaskParams: &protocol.WallstreetNewsTaskParams{
//   Channels: []string{"a-stock-channel", "us-stock-channel", "hk-stock-channel", "goldc-channel%2Coil-channel%2Ccommodity-channel"},
//   Limit:    3,
// },
message WallstreetNewsTaskParams {
  repeated string channels = 1;
  int32 limit = 2;
}

message Wublock123TaskParams {
  repeated string channels = 1;
  repeated string search_keywords = 2;
  int32 pages = 3;
}

message ZsxqTaskParams {
  int32 count_per_request = 1;
}

message WisburgParams {
  enum ChannelType {
    CHANNEL_TYPE_UNSPECIFIED = 0;
    // Posts from https://wisburg.com/viewpoint
    CHANNEL_TYPE_VIEWPOINT = 1;
    // Posts from https://wisburg.com/research
    CHANNEL_TYPE_RESEARCH = 2;
  } 
  // Specify what channels to crawl for Wisburg.
  repeated ChannelType channel_type = 1;
}

// Caus news has 4 items per page, using timestamp as cursor to query next batch
message CaUsNewsTaskParams {
  int32 max_pages = 1;
  int32 lanmu_id = 2;
}

message CustomizedCrawlerParams {
  string crawl_url = 1; // url to crawl, e.g. https://www.cls.cn/telegraph
  string base_selector = 2; // base selector should return a list of DOM elements where each one corresponds to a single post
  optional string title_relative_selector= 3; // relative selector to the base selector
  optional string content_relative_selector = 4;
  optional string external_id_relative_selector = 5;
  optional string time_relative_selector = 6; // if not specified, use the cralwed time as content generated time
  optional string image_relative_selector = 7;
  optional string subsource_relative_selector = 8; // how to deal with subsource spec
  optional string origin_url_relative_selector = 9; // by default is the crawl_url 
  optional bool origin_url_is_relative_path = 10; // if the origin_url_relative_selector generates relative path to crawl_url
}
