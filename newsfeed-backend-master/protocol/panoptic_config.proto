syntax = "proto3";
package protocol;

import "panoptic.proto";

option go_package = "github.com/rnr-capital/newsfeed-backend/publisher/protocol";

message PanopticConfig {
  // Name of this config.
  string name = 1;

  // Which data collector this config is invoking.
  PanopticTask.DataCollectorId data_collector_id = 2;

  // parameters that this task should take in.
  TaskParams task_params = 3;

  // TaskSchedule defines the schedule by which this config should run.
  TaskSchedule task_schedule = 4;

  // If dry_run is set to true, the actual PanopticJob generated from this
  // config will have debug == true. When calling lambda, the data collector
  // will not actually publish the Crawled message to SNS, but output to stdout
  // instead.
  // Usually, before publishing your new crawler, you want to set this field to 
  // true, monitor it in Datadog for a while and then turn it to false to 
  // actually enable it and publish messages into SNS.
  bool dry_run = 5;
}

message TaskSchedule {
  // Start the task immediately, ignoring the schedule.
  bool start_immediatly = 1;

  // There are many schdules that we should be able to support, such as every
  //other duration of time, at certain time of day.
  oneof schedule {
    Routinely routinely = 2;

    // TODO(chenweilunster): Add other types of schedule if necessary.
  }
}

// Routinely defines a schedule that executes every other duration of time.
message Routinely {
  int64 every_milliseconds = 1;
}

// This message is used for the purpose of config push for the scheduler.
message PanopticConfigs {
  // A list of task configs that's used to configure scheduler.
  repeated PanopticConfig config = 1;
}
