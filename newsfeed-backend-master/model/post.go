package model

import (
	"encoding/json"
	"time"

	"github.com/lib/pq"
	"github.com/pgvector/pgvector-go"
	"gorm.io/gorm"
)

/*

Post is a piece of news crawler fetched

Id: primary key, use to identify a sub-source
CreatedAt: time when entity is created
DeletedAt: time when entity is deleted

Title: post's title in plain text
Content: post's content in plain text
SourceID:
Source: source website for example "twitter", "weibo", "Caixin",  "belongs-to" relation
SubSourceID:
SubSource: for example a twitter user, weibo user, sub channel in Caixin etc., "belongs-to" relation

SharedFromPostID:
SharedFromPost:
		if the post is a user shared(like re-twitt) post, set this as the Post originally shared. Support multi-level sharing.
		also if the post is shared:
			Source is a FAKE one representing "shared".
				Name is "分享"
			SubSource representing "user".
				CreatorID is user_id
				Creator is User
				Name is user's name
				ExternalIdentifier is empty


ByUser: mark when user save the post, "many-to-many" relation
PublishedFeeds: feeds that this post published to, "many-to-many" relation

Cursor: The auto-inc global-unique index to keep the relative order of posts

ImageUrlsInJson: Urls for the attached images, in json format string

FileUrlsInJson: Urls for the attached files, in json format string

CrawledAt: The time stamp when crawler generate the post

OriginUrl: The origin_url is the url of crawled website example: "http://companies.caixin.com/2021-04-10/101688620.html"

ContentGeneratedAt: The actual time content is generated by 3rd party website

InSharingChain: If this post is some other post retweets.
	the in-chain posts will not be published as an independent post in a feed, however,
	however, its content will be used to publish/match its root post in the chain.

DeduplicateId: A hash to deduplicate across posts, generated by crawler

SemanticHashing: A hash with the property that similar content will be hashed
into near neighbor in Hamming space. It is a 128 bit binary string.
*/

type Post struct {
	Id                 string `gorm:"primaryKey"`
	CreatedAt          time.Time
	DeletedAt          gorm.DeletedAt
	Title              string    `json:"title"`
	Content            string    `json:"content"`
	SubSourceID        string    `gorm:"constraint:OnUpdate:CASCADE,OnDelete:SET NULL;"`
	SubSource          SubSource `gorm:"constraint:OnUpdate:CASCADE,OnDelete:SET NULL;"`
	SharedFromPostID   *string   `json:"shared_from_post_id"`
	SharedFromPost     *Post     `json:"shared_from_post"`
	ReadByUser         []*User   `json:"read_by_user" gorm:"many2many:user_post_reads;"`
	PublishedFeeds     []*Feed   `json:"published_feeds" gorm:"many2many:post_feed_publishes;constraint:OnDelete:CASCADE;"`
	Cursor             int32     `gorm:"autoIncrement"`
	CrawledAt          time.Time `json:"crawled_at"`
	OriginUrl          string    `json:"origin_url"`
	ContentGeneratedAt time.Time `json:"content_generated_at"`
	InSharingChain     bool      `json:"in_sharing_chain"`

	// A post could be within a reply thread, this field stored all ancestors of
	// this Post, in a chronological order. Under the hood this is tracked via a
	// many to many table of Post.
	// The first Post in ReplyThread is a post that doesn't reply to anything (
	// root post in this thread.)
	// Note that we explicitly not storing this thread in a recursive way similar
	// to SharedFromPost, mostly because GraphQL & GORM doesn't have a way of
	// recursive loading. This would be fine for SharedFromPost because we limit
	// it to be only 2 layers but would make the loading for thread impossible
	// due to the fact that we don't know how long is this thread.
	ReplyThread []*Post `json:"reply_thread" gorm:"many2many:reply_thread;constraint:OnDelete:CASCADE;"`

	// TODO: convert json to array when serve graphql API and ingest from crawler
	ImageUrls pq.StringArray `gorm:"type:TEXT[]" json:"image_urls"`
	FileUrls  pq.StringArray `gorm:"type:TEXT[]" json:"file_urls"`

	DeduplicateId   string           `json:"deduplicate_id"`
	SemanticHashing string           `json:"semantic_hashing"`
	Embedding       *pgvector.Vector `json:"embedding" gorm:"type:vector(100)"`
	Tag             string           `json:"tag"`
	IsRead          bool             `json:"is_read"`
	Delayed         bool             `json:"delayed" gorm:"-" sql:"-"`
}

type Alias Post // Create an alias to avoid recursive call to MarshalJSON
type marshalablePost struct {
	*Alias
	Embedding []float32
}

// customize the marshal process b/c pgvector pointer will mess up the json marshaller
func (p Post) MarshalJSON() ([]byte, error) {

	var emb []float32 = nil
	if p.Embedding != nil {
		emb = p.Embedding.Slice()
	}
	return json.Marshal(&marshalablePost{
		Alias:     (*Alias)(&p),
		Embedding: emb,

	})
}

func (p *Post) UnmarshalJSON(data []byte) error {
	unmarsh := marshalablePost{}
	if err := json.Unmarshal(data, &unmarsh); err != nil {
		return err
	}
	// didn't find a easier way in golang, very verbose
	aux := unmarsh.Alias
	p.Id = aux.Id
	p.CreatedAt = aux.CreatedAt
	p.DeletedAt = aux.DeletedAt
	p.Title = aux.Title
	p.Content = aux.Content
	p.SubSourceID = aux.SubSourceID
	p.SubSource = aux.SubSource
	p.SharedFromPostID = aux.SharedFromPostID
	p.SharedFromPost = aux.SharedFromPost
	p.ReadByUser = aux.ReadByUser
	p.PublishedFeeds = aux.PublishedFeeds
	p.Cursor = aux.Cursor
	p.CrawledAt = aux.CrawledAt
	p.OriginUrl = aux.OriginUrl
	p.ContentGeneratedAt = aux.ContentGeneratedAt
	p.InSharingChain = aux.InSharingChain
	p.ReplyThread = aux.ReplyThread
	p.ImageUrls = aux.ImageUrls
	p.FileUrls = aux.FileUrls
	p.DeduplicateId = aux.DeduplicateId
	p.SemanticHashing = aux.SemanticHashing
	p.Tag = aux.Tag
	p.IsRead = aux.IsRead
	p.Delayed = aux.Delayed

	if unmarsh.Embedding == nil {
		p.Embedding = nil
	} else {
		vec := pgvector.NewVector(unmarsh.Embedding)
		p.Embedding = &vec
	}

	return nil
}
